{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS503_Assignment1_Q3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rMomynlgu9H"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
        "import string\n",
        "import time\n",
        "import numpy as np\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1HVARLsoQZT"
      },
      "source": [
        "First we need to set up the Kaggle api for downloading Kaggle datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxHwcgUg8kum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d319c4-69c2-4b35-d0b7-f14f67404967"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!pip install datasketch[scipy]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasketch[scipy]\n",
            "  Downloading datasketch-1.5.3-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from datasketch[scipy]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from datasketch[scipy]) (1.4.1)\n",
            "Installing collected packages: datasketch\n",
            "Successfully installed datasketch-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9zGRg3WB1y3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "e3ddccae-9016-41a0-8fe1-90e0261c0478"
      },
      "source": [
        " # upload the kaggle.json file provided\n",
        " # if doesn't work for some reason see https://www.kaggle.com/general/74235 to create new api key\n",
        "\n",
        " from google.colab import files \n",
        " files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11ed177e-f2d4-40d1-91d8-ec3babfee208\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11ed177e-f2d4-40d1-91d8-ec3babfee208\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ujandeb\",\"key\":\"9de9f735d28f4c4507a802d6fd74c937\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_999kGogEzz"
      },
      "source": [
        "# create a directory for the kaggle api key\n",
        "\n",
        "!mkdir ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD-Z_9uwoZh0"
      },
      "source": [
        "Next we download the nips-papers dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJfAd20fgQmd",
        "outputId": "aea79531-48ef-4046-f35e-d432e1adbb9e"
      },
      "source": [
        "# download and unzip the nips-papers dataset\n",
        "!kaggle datasets download -d benhamner/nips-papers\n",
        "!unzip nips-papers.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nips-papers.zip to /content\n",
            " 93% 132M/141M [00:01<00:00, 118MB/s]\n",
            "100% 141M/141M [00:01<00:00, 117MB/s]\n",
            "Archive:  nips-papers.zip\n",
            "  inflating: authors.csv             \n",
            "  inflating: database.sqlite         \n",
            "  inflating: paper_authors.csv       \n",
            "  inflating: papers.csv              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRuWmNfoekY"
      },
      "source": [
        "We create a pandas dataframe to store all papers (year, title and abstract)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00j4p9aFlKqL"
      },
      "source": [
        "# read nips papers into a pandas dataframe\n",
        "df = pd.read_csv('papers.csv')\n",
        "\n",
        "df = df.drop(columns=['id', 'event_type', 'pdf_name', 'paper_text'])  # removing redundant columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne1MPxZJo0IS"
      },
      "source": [
        "Here's what it looks like :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aEFpfAcKowXj",
        "outputId": "ce50019f-6938-4846-ba7f-b4cbb6b76a8d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year                                              title          abstract\n",
              "0  1987  Self-Organization of Associative Database and ...  Abstract Missing\n",
              "1  1987  A Mean Field Theory of Layer IV of Visual Cort...  Abstract Missing\n",
              "2  1988  Storing Covariance by the Associative Long-Ter...  Abstract Missing\n",
              "3  1994  Bayesian Query Construction for Neural Network...  Abstract Missing\n",
              "4  1994  Neural Network Ensembles, Cross Validation, an...  Abstract Missing"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W733qHJhxAi8"
      },
      "source": [
        "Data Collection :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8pnTXlro3Eu"
      },
      "source": [
        "Next we need to extract the titles and abstracts of papers from NIPS 2017 and 2020"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eEREHgwy3kj"
      },
      "source": [
        "# function that extracts titles and articles of nips papers from the website given the year\n",
        "def get_title_and_abstract(year):\n",
        "\n",
        "  base_url = 'https://papers.nips.cc'\n",
        "  page = requests.get(base_url+'/paper/'+str(year)) # page url\n",
        "\n",
        "  # using beautifulsoup to get extract page contents\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  paper_list = soup.find_all('li')[2:]  # papers start from the 3rd item\n",
        "  df_list = []  # list to store title and abstracts\n",
        "  for item in paper_list:\n",
        "    title = item.select('a')[0].get_text()  # extracting the title of the paper\n",
        "    link = item.select('a')[0].get('href')  # extracting the partial link of the paper\n",
        "    paper_page = requests.get(base_url+link)  # geting the page of the paper\n",
        "    psoup = BeautifulSoup(paper_page.content, 'html.parser')\n",
        "    abstract = psoup.find_all('p')[2].get_text() # abstract of the paper\n",
        "    df_list.append([year, title, abstract])\n",
        "\n",
        "  return df_list\n",
        "\n",
        "# get 2017 and 2020 papers (can take a long time to run at times)\n",
        "df_2017 = pd.DataFrame(get_title_and_abstract(2017), columns=['year', 'title', 'abstract']) \n",
        "df_2020 = pd.DataFrame(get_title_and_abstract(2020), columns=['year', 'title', 'abstract']) \n",
        "\n",
        "# merge all dataframes\n",
        "df_lat = df_2017.append(df_2020, ignore_index=True)\n",
        "df = df.append(df_lat, ignore_index=True)\n",
        "df = pd.DataFrame().append([df, df_2017,df_2020, ignore_index = True])\n",
        "\n",
        "# removing papers with no abstract\n",
        "df = df[df.abstract != 'Abstract Missing']\n",
        "\n",
        "# remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# reset index\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiHlCil2b-P3"
      },
      "source": [
        "df = pd.DataFrame().append([df, df_2017,df_2020])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1n2Me6hxJVV"
      },
      "source": [
        " Pre-processing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2MDRywixMAO"
      },
      "source": [
        "We create word tokens to use as shingles from the papers using their title and abstracts. We convert each paper (title, abstract) into a set of word tokens (shingles) and use minhash to create signatures. We use the datasketch library for this : http://ekzhu.com/datasketch/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDZpZqthxuxL"
      },
      "source": [
        "perm = 256  # number of permutations for minhash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpkmgF6T_3W4"
      },
      "source": [
        "set_dict = {} # dict for sets of shingles corresponding to each paper (index -> {token1, token2,..})\n",
        "minhash_dict = {} # dict for minhashes corresponding to each set (index -> minhash) [same index as above]\n",
        "for index, row in df.iterrows():\n",
        "  text = row.title + ' ' + row.abstract # concatenating title and abstract\n",
        "  text = text.lower() # lowercase\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))  # removing punctuation\n",
        "  shingles = set(text.split())  # spliting into words and creating set\n",
        "  set_dict[index] = shingles  # (index -> {token1, token2,..})\n",
        "  m = MinHash(num_perm=perm)  # initializing minhash\n",
        "  for d in shingles:\n",
        "    m.update(d.encode('utf8'))\n",
        "  minhash_dict[index] = m # (index -> minhash)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyW0LEjFyEsl"
      },
      "source": [
        "Build LSH:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0f9r8bTyHT0"
      },
      "source": [
        "Building a LSH data structure to store each paper from 1987-2016"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W32sT1jVyDls"
      },
      "source": [
        "thresh = 0.3  # the Jaccard similarity threshold for LSH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1p9SwfJlcBY"
      },
      "source": [
        "# create LSH index\n",
        "lsh = MinHashLSH(threshold=thresh, num_perm=perm)\n",
        "\n",
        "for key, value in minhash_dict.items():\n",
        "  #print(key)\n",
        "  if df.iloc[[key]].year.item() != 2017 and df.iloc[[key]].year.item() != 2020: # ignoring 2017 and 2020 papers\n",
        "    lsh.insert(key, value)  # inserting paper signatures into the LSH data structure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riufbowe2S7y"
      },
      "source": [
        "Original Papers in 2017 : Find top 5 unique or original papers in 2017, i. e. papers which  had  least  similarity  to  papers  in  previous  years (1987-2016)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0jsewpltbXE"
      },
      "source": [
        "We query with all the papers from 2017 in order to find the top 5 unique ones. This requires tuning the Jaccard similarity threshold. We first tune the threshold to find 2017 papers which have Jaccard similarity less than the threshold for all previous year papers. That is, they are different from all previous year papers given the threshold. This gives us the papers that are the most unique compared to previous years. We find that for a threshold of 0.35 there are 76 such papers from 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao8v6_PjnGP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6980f8c-6a53-4f3c-c500-b7a4734f033a"
      },
      "source": [
        "thresh = 0.35  # the Jaccard similarity threshold for LSH\n",
        "\n",
        "# create LSH index\n",
        "lsh = MinHashLSH(threshold=thresh, num_perm=perm)\n",
        "\n",
        "for key, value in minhash_dict.items():\n",
        "  if df.iloc[[key]].year.item() != 2017 and df.iloc[[key]].year.item() != 2020: # ignoring 2017 and 2020 papers\n",
        "    lsh.insert(key, value)  # inserting paper signatures into the LSH data structure\n",
        "\n",
        "indices_2017 = []\n",
        "for index, row in df.iterrows():\n",
        "  if row['year'] == 2017:\n",
        "    m = minhash_dict[index]\n",
        "    result = lsh.query(m)\n",
        "    if result == []: indices_2017.append(index)\n",
        "\n",
        "print(len(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MJyiWI5J9H"
      },
      "source": [
        "Next, we lower the threshold to 0.25 and query with the 76 papers found in the previous step. Each result is a list of papers it matched with, with the given Jaccard similarity (approximately, since LSH). We order the results in the increasing order of their lengths. That is, the first result has the lowest number of matches. This means the first paper in the sorted list is the most unique followed by the 2nd one and so on. We take the first 5 results as our required top 5 unique papers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L5ikNaSvrVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9bd48a1-1f71-4a25-a025-dbfacbbaf175"
      },
      "source": [
        "thresh = 0.25  # the Jaccard similarity threshold for LSH\n",
        "\n",
        "# create LSH index\n",
        "lsh = MinHashLSH(threshold=thresh, num_perm=perm)\n",
        "\n",
        "for key, value in minhash_dict.items():\n",
        "  #print(key)\n",
        "  if df.iloc[[key]].year.item() != 2017 and df.iloc[[key]].year.item() != 2020: # ignoring 2017 and 2020 papers\n",
        "    lsh.insert(key, value)  # inserting paper signatures into the LSH data structure\n",
        "\n",
        "lengths = []  # to store lengths of results\n",
        "indices = []  # to store the indices corresponding to the results\n",
        "for index in indices_2017:\n",
        "  m = minhash_dict[index]\n",
        "  result = lsh.query(m)\n",
        "  lengths.append(len(result))\n",
        "  indices.append(index)\n",
        "\n",
        "for idx in list(np.argsort(lengths)[:5]):\n",
        "  print(df.iloc[[indices[idx]]]['title'].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decomposable Submodular Function Minimization: Discrete and Continuous\n",
            "Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues\n",
            "Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding\n",
            "On-the-fly Operation Batching in Dynamic Computation Graphs\n",
            "Modulating early visual processing by language\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I16gIr6disHV"
      },
      "source": [
        "Classic  Papers:  Our  goal  now  is  to  find  the  top  5  oldest,  classic  papers  in  NIPS  proceedings \n",
        "whose topics were relevant even in 2020. Design your scoring function and justify its choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKQyoOPDi2qR"
      },
      "source": [
        "We use MinHash LSH Forest to get top 5 matches for all papers in 2020. For each such result we score the paper as follow. For each of the matches for a paper, the score is equal to the number of years of the oldest match from 2020. We sort the scores and take the papers corresponding to the highest 5 scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYkpEsGttYJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fd0c69-c3fa-47e3-a57a-ff7d99832fc4"
      },
      "source": [
        "forest = MinHashLSHForest(num_perm=perm)\n",
        "\n",
        "for key, value in minhash_dict.items():\n",
        "  if df.iloc[[key]].year.item() != 2020:  # excluding 2020 papers\n",
        "    forest.add(key, value)\n",
        "\n",
        "forest.index()  # required according to the documentation\n",
        "\n",
        "def get_score(result):\n",
        "  max_score = -1\n",
        "  for index in result:\n",
        "    year = df.iloc[[index]].year.item() # year corresponding to index\n",
        "    s = (datetime.date(2020, 1, 1) - datetime.date(year, 1, 1)).days // 365 # number of years of the match from 2020\n",
        "    if s > max_score: max_score = s\n",
        "    return max_score\n",
        "\n",
        "scores = []\n",
        "indices_2020 = []\n",
        "for index, row in df.iterrows():\n",
        "  if row['year'] == 2020:\n",
        "    m = minhash_dict[index]\n",
        "    result = forest.query(m, 5)\n",
        "    scores.append(get_score(result))\n",
        "    indices_2020.append(index)\n",
        "\n",
        "for idx in list(np.argsort(scores)[len(scores)-5:]):  # last 5 indices correspond to the highest scores\n",
        "  print(df.iloc[[indices_2020[idx]]]['year'].item())\n",
        "  print(df.iloc[[indices_2020[idx]]]['title'].item())\n",
        "  print(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1987\n",
            "An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification\n",
            " \n",
            "2001\n",
            "Minimax Probability Machine\n",
            " \n",
            "1987\n",
            "New Hardware for Massive Neural Networks\n",
            " \n",
            "1987\n",
            "Neural Net and Traditional Classifiers\n",
            " \n",
            "1987\n",
            "Self-Organization of Associative Database and Its Applications\n",
            " \n"
          ]
        }
      ]
    }
  ]
}